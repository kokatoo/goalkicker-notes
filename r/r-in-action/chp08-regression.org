#+STARTUP: showeverything
#+title: R in Action

* Chapter 8: Regression

| Type of regression       | Typical use                                                                                                                                                                              |
|--------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Simple linear            |                                                                                                                                                                                          |
| Polynomial               | The relationship is modeled as an nth order polynomial.                                                                                                                                  |
| Multiple linear          |                                                                                                                                                                                          |
| Multilevel               | Predicting a response variable from data that have a hierarchical structure (for example, students within classrooms within schools). Also called hierarchical, nested, or mixed models. |
| Multivariate             | One or more explanatory variables.                                                                                                                                                       |
| Logistic                 | Predicting a categorical response variable                                                                                                                                               |
| Poisson                  | Predicting a response variable representing counts                                                                                                                                       |
| Cox proportional hazards | Predicting time to an event (death, failure, relapse)                                                                                                                                    |
| Time-series              | Modeling time-series data with correlated errors.                                                                                                                                        |
| Nonlinear                | The form of the model is nonlinear.                                                                                                                                                      |
| Nonparametric            | The form of the model is derived from the data and not specified a priori.                                                                                                               |
| Robust                   | Predicting a quantitative response variable using an approach that’s resistant to the effect of influential observations.                                                                |

** Section 8.2 OLS regression

   In OLS regression, a quantitative dependent variable is predicted from a
   weighted sum of predictor variables. The weights are parameters estimated
   from the data.

*** OLS model assumptions:  
   
   * Normality:
     The error terms are normally distributed.

   * Independence:
     The dependent variable (Y(i)) are independent of each other.

   * Linearity:
     The dependent variable is linearly related to the independent variables.

   * Homoscedasticity:
     The variance of the dependent variable doesn't vary with the levels of the
     independent variables.
     
   If any of these are violated, the confidence interval and prediction interval will not be reliable.

*** 8.2.1 Fitting regression models with lm()

| Symbol   | Usage                                                                                                                                                                  |
|----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| =~=      | For example, a prediction of ~y~ from ~x~, ~z~, and ~w~ would be coded =y ~ x + z + w=.                                                                                |
| ~+~      | Separates predictor variables.                                                                                                                                         |
| ~:~      | Denotes an interaction between predictor variables. The interaction between ~x~ and ~z~ would be coded =y ~ x + z + x:z=.                                              |
| ~*~      | A shortcut for denoting all possible interactions. The code =y ~ x * z * w= expands to =y ~ x + z + w + x:z + x:w + z:w + x:z:w=.                                      |
| ~^~      | Denotes interactions up to a specified degree. The code =y ~ (x + z + w)^2= expands to =y ~ x + z + w + x:z + x:w + z:w=.                                              |
| ~.~      | A placeholder for all other variables in the data frame except the dependent variable. For example, the code =y ~ .= would expand to =y ~ x + z + w=.                  |
| ~-~      | A minus sign removes a variable from the equation. For example, =y ~ (x + z + w)^2 – x:w= expands to =y ~ x + z + w + x:z + z:w=.                                      |
| ~-1~     | Suppresses the intercept. For example, the formula =y ~ x -1= fits a regression of ~y~ on ~x~, and forces the line through the origin at ~x=0~.                        |
| ~I()~    | Elements are interpreted arithmetically. For example, the code =y ~ x + I((z + w)^2)= would expand to =y ~ x + h=, where ~h~ is a new variable rather than interaction |
| function | Mathematical functions can be used in formulas. For example, =log(y) ~ x + z + w= would predict ~log(y)~ from ~x~, ~z~, and ~w~.                                       |


    Other functions that are useful when fitting linear models:

| Function         | Action                                                                                             |
|------------------+----------------------------------------------------------------------------------------------------|
| ~summary()~      | Displays detailed results for the fitted model                                                     |
| ~coefficients()~ | Lists the model parameters (intercept and slopes) for the fitted model                             |
| ~confint()~      | Provides confidence intervals for the model parameters (95% by default)                            |
| ~fitted()~       | Lists the predicted values in a fitted model                                                       |
| ~residuals()~    | Lists the residual values in a fitted model                                                        |
| ~anova()~        | Generates an ANOVA table for a fitted model, or an ANOVA table comparing two or more fitted models |
| ~vcov()~         | Lists the covariance matrix for model parameters                                                   |
| ~AIC()~          | Prints Akaike’s Information Criterion                                                              |
| ~plot()~         | Generates diagnostic plots for evaluating the fit of a model                                       |
| ~predict()~      | Uses a fitted model to predict response values for a new dataset                                   |

*** 8.2.2 Simple linear regression


#+begin_src R
  fit <- lm(weight ~ height, data = women)
  > summary(fit)

  Call:
  lm(formula = weight ~ height, data = women)

  Residuals:
      Min      1Q  Median      3Q     Max 
  -1.7333 -1.1333 -0.3833  0.7417  3.1167 

  Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
  (Intercept) -87.51667    5.93694  -14.74 1.71e-09 ***
  height        3.45000    0.09114   37.85 1.09e-14 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 1.525 on 13 degrees of freedom
  Multiple R-squared:  0.991,	Adjusted R-squared:  0.9903 
  F-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14

  plot(women$height, women$weight,
       xlab="Height (in inches)",
       ylab="Weight (in pounds)")
  abline(fit)
#+end_src

[[./images/chp08-plot7.png]]

*** 8.2.3 Polynomial regression

#+begin_src R
  fit2 <- lm(weight ~ height + I(height^2), data = women)
  > summary(fit2)

  Call:
  lm(formula = weight ~ height + I(height^2), data = women)

  Residuals:
       Min       1Q   Median       3Q      Max 
  -0.50941 -0.29611 -0.00941  0.28615  0.59706 

  Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
  (Intercept) 261.87818   25.19677  10.393 2.36e-07 ***
  height       -7.34832    0.77769  -9.449 6.58e-07 ***
  I(height^2)   0.08306    0.00598  13.891 9.32e-09 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.3841 on 12 degrees of freedom
  Multiple R-squared:  0.9995,	Adjusted R-squared:  0.9994 
  F-statistic: 1.139e+04 on 2 and 12 DF,  p-value: < 2.2e-16

  plot(women$height, women$weight,
       xlab="Height (in inches)",
       ylab="Weight (in pounds)")
  lines(women$height, fitted(fit2))
#+end_src

[[./images/chp08-plot8.png]]

#+begin_src R
  fit3 <- lm(weight ~ height + I(height^2) +I(height^3), data=women)

  plot(women$height, women$weight,
         xlab = "Height (in inches)",
         ylab = "Weight (in pounds)")
  lines(women$height, fitted(fit3))
#+end_src
  
[[./images/chp08-plot9.png]]

    Using the ~scatterplot()~ function:

#+begin_src R
  library(car)

  scatterplot(weight ~ height, data=women,
              spread=FALSE, smoother.args=list(lty=2), 
              pch=19, main="Women Age 30-39",
              xlab="Height (inches)", ylab="Weight (lbs.)")
#+end_src

[[./images/chp08-plot1.png]]

**** Linear vs. nonlinear models

     Note that this polynomial equation is still considered a linear regression
     because the equation involves a weighted sum of predictor variables
     (coefficients). Even a model such as ~Y = B1log(X1) + B2log(X2)~ is linear.
     However, ~Y = B0 + B1exp(X/B2)~ is nonlinear and can be fit with the ~nls()~
     function.

*** 8.2.4 Multiple linear regression

#+begin_src R
  cols <- c("Murder", "Population", "Illiteracy", "Income", "Frost")
  states <- as.data.frame(state.x77[, cols])

  > cor(states)
                   Murder Population Illiteracy     Income      Frost
    Murder      1.0000000  0.3436428  0.7029752 -0.2300776 -0.5388834
    Population  0.3436428  1.0000000  0.1076224  0.2082276 -0.3321525
    Illiteracy  0.7029752  0.1076224  1.0000000 -0.4370752 -0.6719470
    Income     -0.2300776  0.2082276 -0.4370752  1.0000000  0.2262822
    Frost      -0.5388834 -0.3321525 -0.6719470  0.2262822  1.0000000

  scatterplotMatrix(states, spread = FALSE, 
                    smoother.args = list(lty = 2),
                    main="Scatter Plot Matrix")
#+end_src

[[./images/chp08-plot2.png]]

#+begin_src R
  fit <- lm(Murder ~ Population + Illiteracy + Income + Frost, data = states)
  > summary(fit)

  Call:
  lm(formula = Murder ~ Population + Illiteracy + Income + Frost, 
      data = states)

  Residuals:
      Min      1Q  Median      3Q     Max 
  -4.7960 -1.6495 -0.0811  1.4815  7.6210 

  Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
  (Intercept) 1.235e+00  3.866e+00   0.319   0.7510    
  Population  2.237e-04  9.052e-05   2.471   0.0173 *  
  Illiteracy  4.143e+00  8.744e-01   4.738 2.19e-05 ***
  Income      6.442e-05  6.837e-04   0.094   0.9253    
  Frost       5.813e-04  1.005e-02   0.058   0.9541    
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 2.535 on 45 degrees of freedom
  Multiple R-squared:  0.567,	Adjusted R-squared:  0.5285 
  F-statistic: 14.73 on 4 and 45 DF,  p-value: 9.133e-08
#+end_src

*** 8.2.5 Multiple linear regression with interactions

#+begin_src R
  fit <- lm(mpg ~ hp + wt + hp:wt, data=mtcars)
  > summary(fit)

  Call:
  lm(formula = mpg ~ hp + wt + hp:wt, data = mtcars)

  Residuals:
      Min      1Q  Median      3Q     Max 
  -3.0632 -1.6491 -0.7362  1.4211  4.5513 

  Coefficients:
    Estimate Std. Error t value Pr(>|t|)    
  (Intercept) 49.80842    3.60516  13.816 5.01e-14 ***
  hp          -0.12010    0.02470  -4.863 4.04e-05 ***
  wt          -8.21662    1.26971  -6.471 5.20e-07 ***
  hp:wt        0.02785    0.00742   3.753 0.000811 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 2.153 on 28 degrees of freedom
  Multiple R-squared:  0.8848,	Adjusted R-squared:  0.8724 
  F-statistic: 71.66 on 3 and 28 DF,  p-value: 2.981e-13

  library(effects)
  # plot(effect(term, mod, , xlevels), multiline = TRUE)
  > plot(effect("hp:wt", fit, , list(wt = c(2.2, 3.2, 4.2))), multiline = TRUE)
#+end_src

[[./images/chp08-plot3.png]]

** 8.3 Regression diagnostics

   Although the ~summary()~ function describes the model, it provides no
   information about the statistical assumptions underlying the model.
   Regression diagnostics provides the necessary tools for evaluating the
   appropriateness of the regression model and can help uncover and correct
   problems.

#+begin_src R
  states <- as.data.frame(state.x77[,c("Murder", "Population", "Illiteracy", "Income", "Frost")])
  fit <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
  > confint(fit)
                      2.5 %       97.5 %
  (Intercept) -6.552191e+00 9.0213182149
  Population   4.136397e-05 0.0004059867
  Illiteracy   2.381799e+00 5.9038743192
  Income      -1.312611e-03 0.0014414600
  Frost       -1.966781e-02 0.0208304170
#+end_src

#+begin_src R
  fit <- lm(weight ~ height, data=women)
  par(mfrow=c(2,2))
  plot(fit)
#+end_src

[[./images/chp08-plot4.png]]

   Assumptions of OLS regression:
   
   * Normality

     The QQ plot is a probability plot of the standardized residuals against the
     values that would be expected under normality. If the normality assumption
     is met, the points on the graph should fall on th estraight 45-degree line.

   * Independence
     
     One have to understand how the data is collected and can't infer that from
     the plots.

   * Linearity

     In the Residuals vs Fitted graph, you shouldn't see a clear pattern but
     random pattern.

   * Homoscedasticity

     The points in the Scale-Location graph should be random around a horizontal line.

   Residuals vs. Leverage graph:
   
   The graph provides information about individual observations that you might
   want to investigate. This plot is hard to read and might not be that useful.

   * An outlier is an observation that isn’t predicted well by the fitted
     regression model (that is, has a large positive or negative residual).

   * An observation with a high leverage value has an unusual combination of
     predictor values. That is, it’s an outlier in the predictor space (i.e.
     extreme x values). The dependent variable value isn’t used to calculate an
     observation’s leverage.

   * An influential observation is an observation that has a disproportionate
     impact on the determination of the model parameters. Influential
     observations are identified using a statistic called Cook’s distance, or
     Cook’s D.

#+begin_src R
  fit2 <- lm(weight ~ height + I(height^2), data=women)
  par(mfrow=c(2,2))
  plot(fit2)

  newfit <- lm(weight~ height + I(height^2), data=women[-c(13,15),])
#+end_src

[[./images/chp08-plot5.png]]

#+begin_src R
  cols <- c("Murder", "Population", "Illiteracy", "Income", "Frost")
  states <- as.data.frame(state.x77[, cols])
  fit <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)

  par(mfrow=c(2,2))
  plot(fit)
#+end_src

[[./images/chp08-plot6.png]]

*** 8.3.2 An enhanced approach

    The ~car~ package provides a number of functions:

| Function              | Purpose                                      |
|-----------------------+----------------------------------------------|
| ~qqPlot()~            | Quantile comparisons plot                    |
| ~durbinWatsonTest()~  | Durbin–Watson test for autocorrelated errors |
| ~crPlots()~           | Component plus residual plots                |
| ~ncvTest()~           | Score test for nonconstant error variance    |
| ~spreadLevelPlot()~   | Spread-level plots                           |
| ~outlierTest()~       | Bonferroni outlier test                      |
| ~avPlots()~           | Added variable plots                         |
| ~influencePlot()~     | Regression influence plots                   |
| ~scatterplot()~       | Enhanced scatter plots                       |
| ~scatterplotMatrix()~ | Enhanced scatter plot matrixes               |
| ~vif()~               | Variance inflation factors                   |

**** Normality

#+begin_src R
  library(car) 

  cols <- c("Murder", "Population", "Illiteracy", "Income", "Frost")
  states <- as.data.frame(state.x77[, cols]) 

  fit <- lm(Murder ~ Population + Illiteracy + Income + Frost, data = states)
  qqPlot(fit, labels = row.names(states), 
         id.method = "identify", simulate = TRUE, main = "Q-Q Plot")
#+end_src

[[./images/chp08-plot10.png]]

#+begin_src R
  residplot <- function(fit, nbreaks=10) {

    z <- rstudent(fit)

    hist(z, breaks=nbreaks, freq=FALSE,
         xlab="Studentized Residual",
         main="Distribution of Errors")
    rug(jitter(z), col="brown")

    curve(dnorm(x, mean=mean(z), sd=sd(z)),
          add=TRUE, col="blue", lwd=2)

    lines(density(z)$x,
          density(z)$y, col="red",
          lwd=2, lty=2)

    legend("topright",
           legend = c("Normal Curve", "Kernel Density Curve"),
           lty=1:2, col=c("blue","red"), cex=.7)
  }

  residplot(fit)
#+end_src

[[./images/chp08-plot11.png]]

**** Independence of errors

     The best way to assess whether the dependent variable values are
     independent is from your knowledge of how the data were collected. Time
     series data often display autocorrelation. Durbin-Watson test are able to
     detect such serially correlated errors. This test is less applicable for
     non time-dependent data.

#+begin_src R
  > durbinWatsonTest(fit)
   lag Autocorrelation D-W Statistic p-value
     1      -0.2006929      2.317691   0.282
  Alternative hypothesis: rho != 0
#+end_src

**** Linearity

#+begin_src R
  library(car)

  crPlots(fit)
#+end_src

[[./images/chp08-plot12.png]]

**** Homoscedasticity

#+begin_src R
  library(car)

  > ncvTest(fit)
  Non-constant Variance Score Test 
  Variance formula: ~ fitted.values 
  Chisquare = 1.746514, Df = 1, p = 0.18632

  > spreadLevelPlot(fit)

  Suggested power transformation:  1.209626 
#+end_src

[[./images/chp08-plot13.png]]

*** 8.3.3 Global validation of linear model assumption

#+begin_src R
  library(gvlma)

  gvmodel <- gvlma(fit)
  > summary(gvmodel)

  ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS
  USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:
  Level of Significance =  0.05 

  Call:
   gvlma(x = fit) 

                      Value p-value                Decision
  Global Stat        2.7728  0.5965 Assumptions acceptable.
  Skewness           1.5374  0.2150 Assumptions acceptable.
  Kurtosis           0.6376  0.4246 Assumptions acceptable.
  Link Function      0.1154  0.7341 Assumptions acceptable.
  Heteroscedasticity 0.4824  0.4873 Assumptions acceptable.
#+end_src

*** 8.3.4 Multicollinearity

    The square root of the VIF (Variance Inflation Factor) indicates the degree
    to which the confidence interval for that variable's regression parameter is
    expanded relative to a model with uncorrelated predictors. > 2 indicates a
    multicollinearity problem.

#+begin_src R
  library(car)

  > vif(fit)
  Population Illiteracy     Income      Frost 
    1.245282   2.165848   1.345822   2.082547

  > sqrt(vif(fit)) > 2
  Population Illiteracy     Income      Frost 
       FALSE      FALSE      FALSE      FALSE 
#+end_src

** 8.4 Unusual observations

*** 8.4.1 Outliers

    This only test the most extreme point. You need to delete the point and
    rerun the test.

#+begin_src R
  library(car)

  > outlierTest(fit)
         rstudent unadjusted p-value Bonferroni p
  Nevada 3.542929         0.00095088     0.047544
#+end_src

*** 8.4.2 High-leverage points

#+begin_src R
  hat.plot <- function(fit) {

    p <- length(coefficients(fit))
    n <- length(fitted(fit))

    plot(hatvalues(fit), main="Index Plot of Hat Values")
    abline(h=c(2,3)*p/n, col="red", lty=2)

    identify(1:n, hatvalues(fit), names(hatvalues(fit)))
  }

  hat.plot(fit)
#+end_src

[[./images/chp08-plot14.png]]

*** 8.4.3 Influential observations

    Check for Cook's distance which are > 4/(n-k-1), where n is the sample size
    and k is the number of predictors. The author find a criterion of D=1 more
    useful.

#+begin_src R
  cutoff <- 4/(nrow(states)-length(fit$coefficients)-2)
  
  plot(fit, which=4, cook.levels=cutoff)
  abline(h=cutoff, lty=2, col="red")
#+end_src

[[./images/chp08-plot15.png]]

#+begin_src R
  library(car)

  avPlots(fit, ask=FALSE, id.method="identify")
#+end_src


[[./images/chp08-plot16.png]]

#+begin_src R
library(car)

influencePlot(fit, id.method="identify",
                main="Influence Plot",
                sub="Circle size is proportional to Cook's distance")
#+end_src

[[./images/chp08-plot17.png]]

** 8.5 Corrective measures

*** 8.5.2 Transforming variables

| λ              |    -2 |  -1 |      -0.5 |      0 |     0.5 |    1 |   2 |
|----------------+-------+-----+-----------+--------+---------+------+-----|
| Transformation | 1/Y^2 | 1/Y | 1/sqrt(Y) | log(Y) | sqrt(Y) | None | Y^2 |

#+begin_src R
  library(car)

  > summary(powerTransform(states$Murder))
  bcPower Transformation to Normality 
                Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd
  states$Murder    0.6055           1       0.0884       1.1227

  Likelihood ratio test that transformation parameter is equal to 0
  (log transformation)
                             LRT df     pval
  LR test, lambda = (0) 5.665991  1 0.017297

  Likelihood ratio test that no transformation is needed
                             LRT df    pval
  LR test, lambda = (1) 2.122763  1 0.14512

  > boxTidwell(Murder~Population+Illiteracy,data=states)
             MLE of lambda Score Statistic (z) Pr(>|z|)
  Population       0.86939             -0.3228   0.7468
  Illiteracy       1.35812              0.6194   0.5357

  iterations =  19 
#+end_src

** 8.6 Selecting the “best” regression model

*** 8.6.1 Comparing models

    You can compare the fit of two nested models using the ~anova()~ function.

#+begin_src R
  cols <- c("Murder", "Population", "Illiteracy", "Income", "Frost")
  states <- as.data.frame(state.x77[, cols])

  fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
  fit2 <- lm(Murder ~ Population + Illiteracy, data=states)

  > anova(fit2, fit1)
  Analysis of Variance Table

  Model 1: Murder ~ Population + Illiteracy
  Model 2: Murder ~ Population + Illiteracy + Income + Frost
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
  1     47 289.25                           
  2     45 289.17  2  0.078505 0.0061 0.9939

  fit1 <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
  fit2 <- lm(Murder ~ Population + Illiteracy, data=states)
#+end_src

    Models with samller AIC values are preferred. Note AIC doesn't require
    nested models.

#+begin_src R
  > AIC(fit1,fit2)
  df      AIC
  fit1  6 241.6429
  fit2  4 237.6565
#+end_src

*** 8.6.2 Variable selection

#+begin_src R
  library(MASS)

  cols <- c("Murder", "Population", "Illiteracy", "Income", "Frost")
  states <- as.data.frame(state.x77[, cols])

  fit <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)

  > stepAIC(fit, direction="backward")
  Start:  AIC=97.75
  Murder ~ Population + Illiteracy + Income + Frost

               Df Sum of Sq    RSS     AIC
  - Frost       1     0.021 289.19  95.753
  - Income      1     0.057 289.22  95.759
  <none>                    289.17  97.749
  - Population  1    39.238 328.41 102.111
  - Illiteracy  1   144.264 433.43 115.986

  Step:  AIC=95.75
  Murder ~ Population + Illiteracy + Income

               Df Sum of Sq    RSS     AIC
  - Income      1     0.057 289.25  93.763
  <none>                    289.19  95.753
  - Population  1    43.658 332.85 100.783
  - Illiteracy  1   236.196 525.38 123.605

  Step:  AIC=93.76
  Murder ~ Population + Illiteracy

               Df Sum of Sq    RSS     AIC
  <none>                    289.25  93.763
  - Population  1    48.517 337.76  99.516
  - Illiteracy  1   299.646 588.89 127.311

  Call:
  lm(formula = Murder ~ Population + Illiteracy, data = states)

  Coefficients:
  (Intercept)   Population   Illiteracy  
    1.6515497    0.0002242    4.0807366  
#+end_src

**** All subsets regression

     It is widely suggested that a Cp statistic  close to the number of model
     parameters (including the intercept) is a good model.

#+begin_src R
  library(leaps)

  cols <- c("Murder", "Population", "Illiteracy", "Income", "Frost")
  states <- as.data.frame(state.x77[,cols])
  leaps <-regsubsets(Murder ~ Population + Illiteracy + Income + Frost, data=states, nbest=4)

  plot(leaps, scale="adjr2")

  library(car)

  subsets(leaps, statistic="cp",
          main="Cp Plot for All Subsets Regression")
  abline(1,1,lty=2,col="red")
#+end_src

[[./images/chp08-plot18.png]]

** 8.7 Taking the analysis further

*** 8.7.1 Cross-validation

    When description is your primary goal, the selection and interpretation of a
    regression model is adequate. However, if prediction is the goal, you need
    cross validation.

    The ~shrinkage()~ function below uses the ~crossval()~ function in the
    ~bootstrap~ package.

#+begin_src R
  shrinkage <- function(fit, k=10){
    require(bootstrap)

    theta.fit <- function(x,y) { lsfit(x,y) }
    theta.predict <- function(fit,x) { cbind(1,x)%*%fit$coef }

    x <- fit$model[,2:ncol(fit$model)]
    y <- fit$model[,1]

    results <- crossval(x, y, theta.fit, theta.predict, ngroup=k)

    r2 <- cor(y, fit$fitted.values)^2
    r2cv <- cor(y, results$cv.fit)^2

    cat("Original R-square =", r2, "\n")
    cat(k, "Fold Cross-Validated R-square =", r2cv, "\n")
    cat("Change =", r2-r2cv, "\n")
  }

  cols  <- c("Murder", "Population", "Illiteracy", "Income", "Frost")
  states <- as.data.frame(state.x77[, cols])
  fit <- lm(Murder ~ Population + Income + Illiteracy + Frost, data=states)

  > shrinkage(fit)
  Original R-square = 0.5669502 
  10 Fold Cross-Validated R-square = 0.37237 
  Change = 0.1945803 

  fit2 <- lm(Murder ~ Population + Illiteracy, data=states)
  > shrinkage(fit2)
  Original R-square = 0.5668327 
  10 Fold Cross-Validated R-square = 0.5036703 
  Change = 0.06316236
#+end_src

*** 8.7.2 Relative importance

    We have been asking "Which variables are useful for predicting the
    outcome?". But more often we are more interested which variables are the
    /most important/ in predicting the outcome. If predictor variables were
    uncorrelated, this would be simple. But often, the predictors are correlated
    with each other.

    The simplest is to compare the standardized regression coefficients.

#+begin_src R
  cols <- c("Murder", "Population", "Illiteracy", "Income", "Frost")
  states <- as.data.frame(state.x77[, cols])
  zstates <- as.data.frame(scale(states))

  zfit <- lm(Murder~Population + Income + Illiteracy + Frost, data=zstates)
  > coef(zfit)
    (Intercept)    Population        Income    Illiteracy         Frost 
  -2.054026e-16  2.705095e-01  1.072372e-02  6.840496e-01  8.185407e-03 
#+end_src

    The /relative weights/ method approximates the average increase in R-square
    obtained by adding a predictor variable across all possible submodels.

#+begin_src R
  relweights <- function(fit,...){
    R <- cor(fit$model)
    nvar <- ncol(R)
    rxx <- R[2:nvar, 2:nvar]
    rxy <- R[2:nvar, 1]
    svd <- eigen(rxx)
    evec <- svd$vectors
    ev <- svd$values
    delta <- diag(sqrt(ev))
    lambda <- evec %*% delta %*% t(evec)
    lambdasq <- lambda ^ 2
    beta <- solve(lambda) %*% rxy
    rsquare <- colSums(beta ^ 2)
    rawwgt <- lambdasq %*% beta ^ 2
    import <- (rawwgt / rsquare) * 100
    import <- as.data.frame(import)
    row.names(import) <- names(fit$model[2:nvar])
    names(import) <- "Weights"
    import <- import[order(import),1, drop=FALSE]

    dotchart(import$Weights, labels=row.names(import),
             xlab="% of R-Square", pch=19,
             main="Relative Importance of Predictor Variables",
             sub=paste("Total R-Square=", round(rsquare, digits=3)),
             ...)
    return(import)
  }

  cols <- c("Murder", "Population", "Illiteracy", "Income", "Frost")
  states <- as.data.frame(state.x77[, cols])

  fit <- lm(Murder ~ Population + Illiteracy + Income + Frost, data=states)
  > relweights(fit, col="blue")
               Weights
  Income      5.488962
  Population 14.723401
  Frost      20.787442
  Illiteracy 59.000195
#+end_src

[[./images/chp08-plot19.png]]


